¿Qué es Ollama?

¿Qué es FastAPI?

¿Qué es el modelo deepseek-r1?

Uso de peticiones con stream=True

¿Cómo garantizar la escalabilidad de una API que consume modelos de IA pesados?

¿Qué parámetros de Ollama (ej: num_ctx, temperature) afectan el rendimiento/calidad?

¿Qué estrategias usar para balancear carga entre múltiples instancias de Ollama?

¿Qué patrones de diseño (ej: CQRS, Singleton) son útiles para integrar modelos de IA en backend?

